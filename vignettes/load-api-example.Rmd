---
title: "Insight API upload to SciDB example"
author: "Paradigm4 Customer Solutions"
output:
  html_document: default
  pdf_document:
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
library(scidb4gh)
```

# Initialization

The initialization steps are provided below. 
```{r init}
library(scidb4gh)
options(scidb4gh.use_scidb_ee=FALSE)
gh_connect()
```
# Loading Annotation entities

`Annotation` data is different from `Metadata` in that annotations might be shared across studies. Genes, transcripts, and probesets used in gene-expression by micro-array are some examples of annotation data. 

First let us check the mandatory fields for the two entities for annotation data -- `FeatureSet` and `Feature`:

```{r annotation_bloc1, eval=TRUE, include=TRUE}
mandatory_fields()[['FEATURESET']]
mandatory_fields()[['FEATURE']]
```

Now, let us create a `FeatureSet` entity to hold our features:

```{r annotation_bloc2, eval=TRUE, include=TRUE}
featureset_df = data.frame(referenceset_id=-1, # This information is a placeholder for now 
                       name = "FeatureSet1",
                       description = "...",
                       source_uri = "http://xxx.yyy/zzz"
                       )
register_featureset(featureset_df, only_test = TRUE)
```

Next, we register the `FeatureSet` and record the system assigned `featureset_id`:

```{r annotation_bloc3, eval=TRUE, include=TRUE}
featureset_id = register_featureset(featureset_df)
```

Finally, we register a few `Feature`-s within this `FeatureSet`. We use gene information from http://www.ensembl.org/, and fill up some fields as per [GA4GH definitions](http://ga4gh-schemas.readthedocs.io/en/latest/schemas/sequence_annotations.proto.html).

```{r ontology_bloc2, eval=TRUE, include=TRUE}
ontology_id = register_ontology_term(data.frame(term="forward strand", 
                                  source_name="...",
                                  source_version="..."))
```

```{r annotation_bloc4, eval=TRUE, include=TRUE}
feature_df = data.frame(name=c('ENSG00000254029',
                               'ENSG00000250939',
                               'ENSG00000225275'))
feature_df$gene_symbol = c('IGLC4', 'AC034198.7', 'NUP210P2')
feature_df$featureset_id = featureset_id
feature_df$chromosome = c('22', '3', '3')
feature_df$start = c('22910828', '12850659', '11900011')
feature_df$end = c('22911075', '12850860', '11901245')

feature_df$strand_term = ontology_id
feature_df$feature_type = 'gene'
feature_df$source = 'http://www.ensembl.org/'

# Flex fields
feature_df$description = c('immunoglobulin lambda constant 4 (pseudogene)',
                           'NA',
                           'nucleoporin 210 pseudogene 2')
feature_df$hgnc_symbol = c('5858', 'NA', '42707')
register_feature(feature_df, only_test = TRUE)
```

Now let us register the features:
```{r annotation_bloc5, eval=TRUE, include=TRUE}
feature_id = register_feature(feature_df)
print(feature_id)
```


# Loading `Metadata` entities

In this section, we show how to load data of type `Metadata` (i.e. `Project`, `Dataset`, `Individual` etc.) into the pan-study repository. 
All metadata entities except `Project` are versioned by the version of the study (`Dataset`) they belong to. For non-versioned studies,
all entities will have `dataset_version = 1`. 

## Project

Here we work on creating a test project entry. First we check the mandatory fields:
```{r project-bloc1}
mandatory_fields()[['PROJECT']]
```

Next we formulate an R dataframe to hold the project information, and test the completeness
of the fields.
```{r project-bloc2}
projectname = paste("Kriti test project", date())
df_project = data.frame(name = projectname,
                        description = "...",
                        flexfield1 = 32,
                        flexfield2 = "asdf")
register_project(df_project, only_test = TRUE)
```

Finally, we load the project into the pan-study repo. If the project already exists in the repo,
the ID for that study would be returned. 
```{r project-bloc3}
project_id = register_project(df = df_project)
print(project_id)
```

You can check that this entry was properly uploaded by using the query API
```{r project-bloc4}
get_projects(project_id = project_id)
```

## Dataset

We follow a similar procedure as above to register a dataset. First, check the mandatory fields:
```{r dataset-bloc1}
mandatory_fields()[['DATASET']]
```

Formulate an R dataframe:
```{r dataset-bloc2}
df_dataset = data.frame(name = "Kriti_test_dataset",
                        description = "...",
                        flexfield1 = "any-value")
try(register_dataset(df_dataset, only_test = TRUE))
```

Let us fix the error, and try to upload the dataset.
```{r dataset_bloc3}
df_dataset$project_id = project_id # the ID retrieved earlier
dataset_record = register_dataset(df = df_dataset)
print(dataset_record)
```

Note that there is an additional entry `dataset_version` in the output.
By default, this was assigned at 1. 

## Individuals

Individuals are one of the first level entities within a study. Let us check the mandatory fields:
```{r individual-bloc1}
mandatory_fields()[['INDIVIDUAL']]
```

Formulate an R dataframe:
```{r individual-bloc2}
dataset_id = dataset_record$dataset_id

register_ontology_term(df = data.frame(term=c("homo sapiens", "male", "female"), 
                                       source_name="...", source_version="..."))
human_id = search_ontology(term = "homo sapiens")
male_id = search_ontology(term = "male")
female_id = search_ontology(term = "female")
df_individual = data.frame(dataset_id = dataset_id,
                            name = c("Jake", "Karen"),
                            description = "...",
                            species_ = human_id, 
                            sex_ = c(male_id, female_id),
                            race = c('Caucasian', 'Asian'),
                           stringsAsFactors = FALSE)
register_individual(df_individual, only_test = TRUE)
```

One thing to note above was that `species_`, `sex_` are mandatory terms that are linked to 
a fixed vocabulary/ontology stored in the repository. They are distinguished from other terms
by the trailing underscore (`_`) and must be populated with integral values corresponding to 
ontology id-s of specific terms. The ontology is searched by the function
`search_ontology(term = ...)` (regular expressions can be used as shown above).

Next we upload the individual.
```{r individual_bloc3}
individual_record = register_individual(df = df_individual)
print(individual_record)
```

Let us add a few more individual entries; this time, we try to register a duplicate. 
```{r individual-bloc4}
df_individual2 = data.frame(dataset_id = dataset_id,
                            name = c("Jake", "Linda"),
                            description = "...",
                            species_ = human_id, 
                            sex_ = c(male_id, female_id),
                            race = c('Caucasian', 'African-American'))
individual_record2 = register_individual(df = df_individual2)
print(individual_record2)
```

Notice how one of the older entries was identified as a duplicate. The system 
still returns the `individual_id` for that entry (this can be a useful feature
when loading new studies)

## Biosample

First, let us check the mandatory fields:
```{r biosample-bloc1}
mandatory_fields()[['BIOSAMPLE']]
```

One important thing is that biosamples must be associated with the `individual_id` from whom
the sample was collected. We achieve this as follows:

```{r biosample-bloc2}
ontology_id = register_ontology_term(df = data.frame(term=c("multiple myeloma"), 
                                                     source_name="...", 
                                                     source_version="..."))
disease_id = search_ontology(term = "multiple myeloma")

ix = search_individuals(dataset_id = dataset_id)
Jake_id = ix[grep("Jake", ix$name), ]$individual_id
Linda_id = ix[grep("Linda", ix$name), ]$individual_id
df_biosample = data.frame(dataset_id = dataset_id,
                            name = c("Jake_Sample1", "Jake_Sample2", "Linda_Sample1"),
                            description = c("visit 1", "visit 2", "visit 1"),
                            disease_ = disease_id, 
                            individual_id = c(Jake_id, Jake_id, Linda_id),
                            collection_location = c('BoneMarrow', 'BoneMarrow', 
                                                    'PeripheralBlood'))
register_biosample(df = df_biosample, only_test = TRUE)
```

Next we upload the biosample.
```{r biosample_bloc3}
biosample_record = register_biosample(df = df_biosample)
print(biosample_record)
```

## ExperimentSet

The ExperimentSet entity provides an overview of all measuerment entities used in a study. This can range from RNASeq (RNAQUANTIFICATION) to CNV (COPYNUMBER). Below we register one experimentSet (RNASeq) that we will further link to two different processing algorithms (see next section).

```{r expSet_bloc1}
mandatory_fields()[['EXPERIMENTSET']]
```

```{r expSet_bloc2}
df_experimentSet = data.frame(dataset_id = dataset_id,
                              name = "RNA-seq",
                              description = "RNA-seq experiments",
                              molecule = "mRNA",
                              measurement_entity = "RNAQUANTIFICATION",
                              stringsAsFactors = FALSE)
register_experimentset(df_experimentSet, only_test = TRUE)
```

Next we create the ExperimentSet entity.

```{r expSet_bloc3}
experimentSet_record = register_experimentset(df_experimentSet)
print(experimentSet_record)

experimentset_id = experimentSet_record$experimentset_id
```

## MeasurementSets

MeasurementSets hold information for data processing pipelines within a study (more details are provided at this GA4GH [link](http://ga4gh-schemas.readthedocs.io/en/latest/api/rna_quantifications.html)). Below, we register two pipelines corresponding to two RNASeq data processing algorithms:
```{r rqset-bloc1}
mandatory_fields()[['MEASUREMENTSET']]
```

```{r rqset_bloc2, eval=TRUE, include=FALSE}
pipeline_df = data.frame(dataset_id = dataset_id,
                         entity = 'RNAQUANTIFICATION',
                         experimentset_id = experimentset_id,
                         name = c('Salmon gene counts', 'Cufflinks_Gene_FPKM'),
                         description = c('Simple matrix file ENSG_ID, defined by ensembl v74 GTF, 
                                         in first column and Salmon counts for each respective specimen 
                                         in successive columns',
                                         'Simple matrix file ENSG_ID, defined by ensembl v74 GTF, 
                                         in first column and Cufflinks FPKM for each respective specimen 
                                         in successive columns'),
                         algorithm = c('Salmon', 'Cufflinks'),
                         units = c('counts', 'FPKM'),
                         featureset_id = featureset_id,
                         feature_type = 'gene')
pipeline_df$description = gsub("\n", "BR", pipeline_df$description)
register_measurementset(pipeline_df, only_test = TRUE)
```

Next we register the pipelines.
```{r rqset_bloc3}
pipeline_record = register_measurementset(df = pipeline_df)
print(pipeline_record)
```


## Other `Metadata` entities

Other `Metadata` entities (e.g. `Experiment`, `VariantSet`, `FusionSet` etc.) can be
loaded into SciDB using corresponding API calls (e.g. `register_experimentset()` etc.)
using similar same steps as described above. 

# Loading Data entities

We illustrate the process of loading RNASeq data from matrix table files that have the following format:

```{r data_bloc1, eval = TRUE, include = TRUE}
read.delim(file = system.file("extdata", "rnaseq.txt", 
                               package = "scidb4gh"))
```

# Upload from server 

These matrix files can sometimes be very large, and we provide an API to directly load the data into SciDB. Note that the file-path used below must exist on the SciDB server. Alternatively, use the client-to-server upload method described further below.

```{r reg-exprn-set, eval = TRUE, include = TRUE}
# Choose the first pipeline for 'Salmon  Gene counts'
measurementset_id = pipeline_record[1, ]$measurementset_id

expr_file_path = system.file("extdata", "rnaseq.txt", 
                                                   package = "scidb4gh")
# register expression data 
upload1Status = tryCatch(
  {
    register_expression_matrix( filepath = expr_file_path,
                            measurementset_id = measurementset_id, 
                            featureset_id = featureset_id,
                            feature_type = 'gene'
                          )
    return(TRUE)
  }, error = function(e) {
    cat("Failed to upload expression matrix file. Make sure following file-path exists on SciDB server:\n\t", 
        expr_file_path,
        "\n")
    return(FALSE)
  }
)
```

## Upload from client to server

```{r}
measurementset = get_measurementsets(measurementset_id = measurementset_id)
featureset = get_featuresets(featureset_id = featureset_id)
register_expression_matrix_client(filepath = expr_file_path, 
                                  measurementset = measurementset,
                                  featureset = featureset,
                                  file_format = 'wide')
```

You can speed up the upload by providing reference feature and biosample dataframes as below

```{r}
bios_ref = get_biosamples()
ftr_ref = get_features()
register_expression_matrix_client(filepath = expr_file_path, 
                                  measurementset = measurementset,
                                  featureset = featureset,
                                  feature_ref = ftr_ref,
                                  biosample_ref = bios_ref,
                                  file_format = 'wide')
```

Now verify the upload:
```{r data_bloc2_verify, eval=TRUE}
measurementset = get_measurementsets(measurementset_id = 
                                                   measurementset_id)
eset = search_rnaquantification(measurementset = measurementset)
print(eset)
exprs(eset)
```

# Dataset versioning

This section illustrates how to handle versioned clinical studies using the load API.
If we want to increment the dataset version from 1 to 2:

```{r dataset_increment, eval = FALSE}
d = search_datasets(project_id = project_id)
d = d[1, ] # in case there are more than 1 datasets in a project, work on them one at a time
print(d)
if (max(d$dataset_version) == 1){
  # Incrementing the dataset version from 1 to 2
  cat("Incrementing the dataset version from 1 to 2\n")
  # d$name = "Kriti_test_dataset"
  d$description = "version 2"
  d$flexfield1  = NULL # drop the original flex field
  d$flexfield_new = "new value here"
  increment_dataset_version(df = d)
} else {
  cat("Dataset version was already incremented from 1 to 2\n")
}
```

Now use the query API to test that two versions were really created

```{r testversioning, eval = FALSE}
dataset_id = dataset_record$dataset_id # From the registration step earlier
get_datasets(dataset_id = dataset_id)
```

By default this returns the latest version. To get an older version:
```{r testversioning2, eval = FALSE}
get_datasets(dataset_id = dataset_id, dataset_version = 1, all_versions = FALSE)
```

You will note that the flex-fields are different in the two cases

Now let us add individuals to this dataset -- we add one new individual `Ram` in this version. 
```{r ver_indiv, eval = FALSE}
df_individual_v2 = data.frame(dataset_id = dataset_id,
                            name = c("Jake", "Karen", "Linda", "Ram"),
                            description = "...",
                            species_ = human_id,
                            sex_ = c(male_id, female_id, female_id, male_id),
                            race = c('Caucasian', 'Asian', 'African-American', 'Asian'))
register_individual(df = df_individual_v2, dataset_version = 2)
```

# Calculate statistics

We provide some simple functions to calculate statistics on the different measurement data types.

### Gene expression data

The function below calculates statistics for gene-expression data across all studies that a user has access to. This is useful as a quality check for the upload. 

```{r}
calculate_statistics_rnaquantification()
# alternatively specify one pipeline id
# calculate_statistics_rnaquantification(measurementset_id = 1)
```

### Variant data

The function below can evaluate statistics for variant data. We do not evaluate it here
since variant data has not been loaded in this example notebook. 

```{r, eval=FALSE}
calculate_statistics_variant()
```